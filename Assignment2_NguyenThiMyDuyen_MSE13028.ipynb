{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_NguyenThiMyDuyen_MSE13028.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhYwmJUDtkaPUIEex1aTpt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trungduyen0220/text-classification-assignment/blob/master/Assignment2_NguyenThiMyDuyen_MSE13028.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i8BVuU72_Ia",
        "colab_type": "text"
      },
      "source": [
        "# Programming Assignment 2: Vietnamese Word Segmentation\n",
        "\n",
        "Nguyễn Thị Mỹ Duyên - 19MSE13028"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILWcaYH_E1px",
        "colab_type": "text"
      },
      "source": [
        "## Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CuOF_nA2yHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "b0197b4e-96d2-4a30-99c1-034584fddb57"
      },
      "source": [
        "!rm -f train.txt\n",
        "!rm -f test.txt\n",
        "!wget https://www.dl.dropboxusercontent.com/s/reor8jnqedk7svt/train.txt\n",
        "!wget https://www.dl.dropboxusercontent.com/s/zp635cd1zhofm62/test.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-18 22:09:41--  https://www.dl.dropboxusercontent.com/s/reor8jnqedk7svt/train.txt\n",
            "Resolving www.dl.dropboxusercontent.com (www.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to www.dl.dropboxusercontent.com (www.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5485066 (5.2M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   5.23M  34.4MB/s    in 0.2s    \n",
            "\n",
            "2020-09-18 22:09:42 (34.4 MB/s) - ‘train.txt’ saved [5485066/5485066]\n",
            "\n",
            "--2020-09-18 22:09:42--  https://www.dl.dropboxusercontent.com/s/zp635cd1zhofm62/test.txt\n",
            "Resolving www.dl.dropboxusercontent.com (www.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to www.dl.dropboxusercontent.com (www.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 871524 (851K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 851.10K  3.11MB/s    in 0.3s    \n",
            "\n",
            "2020-09-18 22:09:43 (3.11 MB/s) - ‘test.txt’ saved [871524/871524]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5spg4_xEsD-",
        "colab_type": "text"
      },
      "source": [
        "Firstly, import necessary packages for the work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZwqmh-X3NhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5950e579-a0c6-40bf-d6e8-9e3f93361054"
      },
      "source": [
        "!pip install sklearn-crfsuite==0.3.6\n",
        "from itertools import chain\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn-crfsuite==0.3.6 in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite==0.3.6) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite==0.3.6) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite==0.3.6) (4.41.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite==0.3.6) (0.9.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UYBeYA4ExzU",
        "colab_type": "text"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM8ZcBr93YK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3143c96f-db69-43bf-df5e-eee2fa29a6a4"
      },
      "source": [
        "def load_data(input_file):    \n",
        "    sentence = []\n",
        "    with open(input_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                continue\n",
        "            wordtags = line.split()\n",
        "            word = wordtags[0], wordtags[1]\n",
        "            sentence.append(word)\n",
        "    return sentence\n",
        "\n",
        "train_data = load_data('./train.txt')\n",
        "test_data = load_data('./test.txt')\n",
        "train_data[0:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Nam', 'B-W'),\n",
              " ('hồn', 'B-W'),\n",
              " ('nhiên', 'I-W'),\n",
              " (':', 'B-W'),\n",
              " ('\"', 'B-W'),\n",
              " ('Tụi', 'B-W'),\n",
              " ('tôi', 'B-W'),\n",
              " ('xài', 'B-W'),\n",
              " ('tiền', 'B-W'),\n",
              " ('ngân', 'B-W')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozlNkB5cE87P",
        "colab_type": "text"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jyd2Fvv8il-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2features(sentence, i):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        sentence (list): list of words [w1, w2,...,w_n]\n",
        "        i (int): index of the word\n",
        "    Return:\n",
        "        features (dict): dictionary of features\n",
        "    \"\"\"\n",
        "    word = sentence[i]\n",
        "    features = {\n",
        "        'is_first': i == 0,\n",
        "        'is_last': i == len(sentence) - 1,\n",
        "        'is_first_capital': word[0].isupper(),\n",
        "        'is_all_caps': int(word.upper() == word),\n",
        "        'is_all_lower': word.lower() == word,\n",
        "        'word': word,\n",
        "        'word.lower()': word.lower(),\n",
        "        'prefix_1': word[0],\n",
        "        'prefix_2': word[:2],\n",
        "        'prefix_3': word[:3],\n",
        "        'prefix_4': word[:4],\n",
        "        'suffix_1': word[-1],\n",
        "        'suffix_2': word[-2:],\n",
        "        'suffix_3': word[-3:],\n",
        "        'suffix_4': word[-4:],\n",
        "        'prev_word': '' if i==0 else sentence[i-1].lower(),\n",
        "        'next_word': '' if i==len(sentence)-1 else sentence[i+1].lower(),\n",
        "        'has_hyphen': '-' in word,\n",
        "        'is_numeric': word.isdigit(),\n",
        "        'capitals_inside': word[1:].lower() != word[1:]\n",
        "    }\n",
        "    return features\n",
        "                                             \n",
        "def sent2features(sentence):\n",
        "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "def sent2labels(sentence):\n",
        "    return [postag for token, postag in sentence]\n",
        "\n",
        "def untag(sentence):\n",
        "    return [token for token, _ in sentence]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXEqCfOvFFEQ",
        "colab_type": "text"
      },
      "source": [
        "Check the first word to see it's features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-U686Ny8k-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "7cebb072-fd84-4110-aee8-35bc3960dbd4"
      },
      "source": [
        "sent2features(untag(train_data))[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'capitals_inside': False,\n",
              " 'has_hyphen': False,\n",
              " 'is_all_caps': 0,\n",
              " 'is_all_lower': False,\n",
              " 'is_first': True,\n",
              " 'is_first_capital': True,\n",
              " 'is_last': False,\n",
              " 'is_numeric': False,\n",
              " 'next_word': 'hồn',\n",
              " 'prefix_1': 'N',\n",
              " 'prefix_2': 'Na',\n",
              " 'prefix_3': 'Nam',\n",
              " 'prefix_4': 'Nam',\n",
              " 'prev_word': '',\n",
              " 'suffix_1': 'm',\n",
              " 'suffix_2': 'am',\n",
              " 'suffix_3': 'Nam',\n",
              " 'suffix_4': 'Nam',\n",
              " 'word': 'Nam',\n",
              " 'word.lower()': 'nam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6OuzWSBFL_O",
        "colab_type": "text"
      },
      "source": [
        "Extract features from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LrnZqet90xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = [sent2features(untag(train_data))]\n",
        "y_train = [sent2labels(train_data)]\n",
        "\n",
        "X_test = [sent2features(untag(test_data))]\n",
        "y_test = [sent2labels(test_data)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR5q7BpTFOeL",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xsdZRyJFQD2",
        "colab_type": "text"
      },
      "source": [
        "Here, I use the model which is optimised by Gradient Descent using the LBGS method with L1 and L2 regularisation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EhC-sKu_kAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "c5c8ca99-0092-40f9-9833-fe8c272cd3a5"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True,\n",
        "    all_possible_states=True,\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=True, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.01, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KDJPRf_FgnF",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Measures\n",
        "\n",
        "Now I will evaluate the trained CRF model on the test data.\n",
        "\n",
        "- P(recision): (number of words correctly segmented)/(number of words in the system output)\n",
        "- R(ecall): (number of words correctly segmented)/(number of words in the reference corpus)\n",
        "- F1 measure = 2*P*R/(P+R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt8Y2wZvBXLB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7391de6-4d11-4807-b838-3a7fc4e19009"
      },
      "source": [
        "y_pred = crf.predict(X_test)\n",
        "print(metrics.flat_accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9524130538088345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQb9RNGEEEa-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f2ec916e-0aaf-4545-a455-eddd630bcc86"
      },
      "source": [
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=crf.classes_, digits=3\n",
        "))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         B-W      0.954     0.978     0.966     62131\n",
            "         I-W      0.949     0.896     0.922     28356\n",
            "\n",
            "    accuracy                          0.952     90487\n",
            "   macro avg      0.952     0.937     0.944     90487\n",
            "weighted avg      0.952     0.952     0.952     90487\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg3VJbYhFxS9",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99hZwQe4F2KO",
        "colab_type": "text"
      },
      "source": [
        "Use the package [seqeval](https://github.com/chakki-works/seqeval) for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swfjD8vtAu2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "0705a911-8450-4209-d6b5-849f52a3069a"
      },
      "source": [
        "!pip install seqeval[cpu]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval[cpu] in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval[cpu]) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval[cpu]) (1.18.5)\n",
            "Requirement already satisfied: tensorflow>=1.13.1; extra == \"cpu\" in /usr/local/lib/python3.6/dist-packages (from seqeval[cpu]) (2.3.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval[cpu]) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval[cpu]) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval[cpu]) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.32.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.35.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (50.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.17.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (1.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (4.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.13.1; extra == \"cpu\"->seqeval[cpu]) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jfwgNGuA2Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def get_tags(filepath):\n",
        "    res = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        cur_sen = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                if len(cur_sen) != 0:\n",
        "                    res.append(cur_sen)\n",
        "                    cur_sen = []\n",
        "            else:\n",
        "                word, tag = line.split()\n",
        "                cur_sen.append(tag)\n",
        "    if len(cur_sen) != 0:\n",
        "        res.append(cur_sen)\n",
        "    return res\n",
        "\n",
        "def evaluate(test_file, output_file):\n",
        "    y_true = get_tags(test_file)\n",
        "    y_pred = get_tags(output_file)\n",
        "\n",
        "    p = precision_score(y_true, y_pred)\n",
        "    r = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    return p, r, f1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQLZ14NNlM0",
        "colab_type": "text"
      },
      "source": [
        "Create two file output.txt and answer.txt and evaluate them with p, r and f1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4vVKoaVHBL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_file(filename, data):\n",
        "  with open(filename, 'w+') as filehandle:\n",
        "    for s in data:\n",
        "      word_output, tag_output = s[0], s[1]\n",
        "      filehandle.writelines(word_output + \"\\t\" + tag_output + \"\\n\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMYiyisAKAr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sentence with predict tag\n",
        "sentence = []\n",
        "for i, index in enumerate(test_data):\n",
        "  word_output, tag_output = index[0], y_pred[0][i]\n",
        "  word = word_output, tag_output\n",
        "  sentence.append(word)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyufjyw4PCzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "702b1206-f568-42c9-e6b8-3adc1c2ce17f"
      },
      "source": [
        "extract_file('output_file.txt', sentence)\n",
        "extract_file('test_file.txt', test_data)\n",
        "evaluate('output_file.txt', 'test_file.txt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9276367674751734, 0.904433111023931, 0.9158879989829647)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}