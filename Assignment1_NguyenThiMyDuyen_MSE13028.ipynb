{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_NguyenThiMyDuyen_MSE13028.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0K3jTHpDvUP2nnh5j1WpE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trungduyen0220/text-classification-assignment/blob/master/Assignment1_NguyenThiMyDuyen_MSE13028.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlzOApSpoQN-",
        "colab_type": "text"
      },
      "source": [
        "# Loading & preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9X01vRHV09G",
        "colab_type": "text"
      },
      "source": [
        "## 1. Downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-tbP5lvgNs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "808c5aa6-288e-4dc5-a1f7-b76299377f78"
      },
      "source": [
        "!rm -f train.csv test.csv\n",
        "!wget https://raw.githubusercontent.com/minhpqn/NLP-Notes/master/data/arxiv/train.csv\n",
        "!wget https://raw.githubusercontent.com/minhpqn/NLP-Notes/master/data/arxiv/test.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-06 18:14:58--  https://raw.githubusercontent.com/minhpqn/NLP-Notes/master/data/arxiv/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10400749 (9.9M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   9.92M  19.7MB/s    in 0.5s    \n",
            "\n",
            "2020-09-06 18:14:58 (19.7 MB/s) - ‘train.csv’ saved [10400749/10400749]\n",
            "\n",
            "--2020-09-06 18:14:58--  https://raw.githubusercontent.com/minhpqn/NLP-Notes/master/data/arxiv/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1133794 (1.1M) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>]   1.08M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-09-06 18:14:59 (11.0 MB/s) - ‘test.csv’ saved [1133794/1133794]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaKQQBWYVVgY",
        "colab_type": "text"
      },
      "source": [
        "## 2. Importing library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF3eKj1vgTmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b96fa4ec-baf5-420a-aa97-503bb9d31b22"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import zipfile\n",
        "import glob\n",
        "from string import digits\n",
        "import random\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "# from keras.layers.embeddings import Embedding\n",
        "# from keras.preprocessing import sequence\n",
        "# from keras.initializers import glorot_uniform\n",
        "\n",
        "from collections import defaultdict\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipXfZj-kWMUU",
        "colab_type": "text"
      },
      "source": [
        "Let's see some rows in the data frame `train_df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALWIBZEWHfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "364bfe1b-3c5c-461c-ca17-4d9be5d7026e"
      },
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901.05003</td>\n",
              "      <td>Quantum Teleportation-Inspired Algorithm for S...</td>\n",
              "      <td>We show that low-depth random quantum circui...</td>\n",
              "      <td>quant-ph</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1903.08905</td>\n",
              "      <td>RAP-Net: Recurrent Attention Pooling Networks ...</td>\n",
              "      <td>The response selection has been an emerging ...</td>\n",
              "      <td>cs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1907.11533</td>\n",
              "      <td>Combinatorial protein-protein interactions on ...</td>\n",
              "      <td>Scaffold proteins organize cellular processe...</td>\n",
              "      <td>q-bio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1711.07582</td>\n",
              "      <td>CVXR: An R Package for Disciplined Convex Opti...</td>\n",
              "      <td>CVXR is an R package that provides an object...</td>\n",
              "      <td>stat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1910.05833</td>\n",
              "      <td>Analytical Solution of (2+1) Dimensional Dirac...</td>\n",
              "      <td>In this article, we studied the system of (2...</td>\n",
              "      <td>quant-ph</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ...  category\n",
              "0  1901.05003  ...  quant-ph\n",
              "1  1903.08905  ...        cs\n",
              "2  1907.11533  ...     q-bio\n",
              "3  1711.07582  ...      stat\n",
              "4  1910.05833  ...  quant-ph\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb3kP_zaVehv",
        "colab_type": "text"
      },
      "source": [
        "## 3. Cleaning and processing text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-hCijZFNkNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preformat(sentence):\n",
        "  \n",
        "  # Clean text data\n",
        "  sentence = sentence.lower() # Lower the text sentence\n",
        "  sentence = sentence.translate(str.maketrans('', '', digits)) # Delete digits number in text sentence\n",
        "  sentence = re.sub(r\"http\\S+\", \"\", sentence) # Remove URL in text sentence\n",
        "  sentence = sentence.translate(str.maketrans('', '', string.punctuation)) # Remove punctuations in text sentence\n",
        "  sentence = sentence.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{}/<>?\\\\|`~=_+'\"\"\"})  # Remove special character in text sentence\n",
        "\n",
        "  # Processing text data with NLP\n",
        "  tokens = word_tokenize(sentence)\n",
        "  tags = nltk.pos_tag(tokens)\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = \"\"\n",
        "  for i, token in enumerate(tokens):\n",
        "    pos_tag = tags[i][1]\n",
        "    if pos_tag.startswith('N'):\n",
        "        lemma = lemmatizer.lemmatize(token, pos=NOUN)\n",
        "        words += lemma + \" \"\n",
        "    elif pos_tag.startswith('V'):\n",
        "        lemma = lemmatizer.lemmatize(token, pos=VERB)\n",
        "        words += lemma + \" \"\n",
        "    elif pos_tag.startswith('J'):\n",
        "        lemma = lemmatizer.lemmatize(token, pos=ADJ)\n",
        "        words += lemma + \" \"\n",
        "    else:\n",
        "        lemma = token\n",
        "        words += \" \"\n",
        "  \n",
        "  return ' '.join(words.split())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZTzPovYQok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "57d0985b-1344-4d40-ad90-1b5c978a146f"
      },
      "source": [
        "# Text before formatting\n",
        "print(\"---Text before formatting: \")\n",
        "print(train_df['abstract'][0])\n",
        "# Text after formatting\n",
        "print(\"---Text after formatting: \")\n",
        "print(preformat(train_df['abstract'][0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Text before formatting: \n",
            "  We show that low-depth random quantum circuits can be efficiently simulated by a quantum teleportation-inspired algorithm. By using logical qubits to redirect and teleport the quantum information in quantum circuits, the original circuits can be renormalized to new circuits with a smaller number of logical qubits. We demonstrate the algorithm to simulate several random quantum circuits, including 1D-chain 1000-qubit 42-depth, 2D-grid 125*8-qubit 42-depth and 2D-Bristlecone 72-qubit 32-depth circuits. Our results present a memory-efficient method with a clear physical picture to simulate low-depth random quantum circuits. \n",
            "---Text after formatting: \n",
            "show lowdepth random quantum circuit be simulate quantum teleportationinspired algorithm use logical qubits redirect teleport quantum information quantum circuit original circuit be renormalize new circuit small number logical qubits demonstrate algorithm simulate several random quantum circuit include dchain qubit depth dgrid qubit depth dbristlecone qubit depth circuit result present memoryefficient method clear physical picture simulate lowdepth random quantum circuit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWTj0TtCWnv7",
        "colab_type": "text"
      },
      "source": [
        "## 4. Save the data into X_train, Y_train, X_test, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uRC4hlLxHq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_csv(filename):\n",
        "    abstract = []\n",
        "    category = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile )\n",
        "\n",
        "        for row in csvReader:\n",
        "            # Using preformat function to processing text data before training\n",
        "            abstract.append(preformat(row[2]))\n",
        "            category.append(row[3])\n",
        "    \n",
        "    # Remove the title\n",
        "    abstract.pop(0)\n",
        "    category.pop(0)\n",
        "    # Transform to array\n",
        "    X = np.asarray(abstract)\n",
        "    Y = np.asarray(category)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = read_csv('train.csv')\n",
        "X_test, Y_test = read_csv('test.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq3FfojVfS9V",
        "colab_type": "text"
      },
      "source": [
        "It takes a litle time to train because of lemmatizer processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WekXIpOLe6WE",
        "colab_type": "text"
      },
      "source": [
        "# Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHOQpxNdflCl",
        "colab_type": "text"
      },
      "source": [
        "In this section, I will implement the Multinomial Naive Bayes (MNB) model.\n",
        "\n",
        "Firstly, extract a vocabulary from a training dataset which is a list of sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7m71COTig5p",
        "colab_type": "text"
      },
      "source": [
        "## 1. Build vocabulary function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJzJxISswAQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(X):\n",
        "    \"\"\"\n",
        "    Build vocabulary from dataset\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for sentences in X:\n",
        "        for word in sentences.split():\n",
        "            # Check if word is a punctuation\n",
        "            \n",
        "            if word in string.punctuation:\n",
        "                continue\n",
        "            if word not in vocab:\n",
        "                idx = len(vocab)\n",
        "                vocab[word] = idx\n",
        "    return vocab"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS9q1nRtf-Qt",
        "colab_type": "text"
      },
      "source": [
        "Let's check how the function `build_vocab` works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY74jGPugBom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "81eaaa74-5ca2-4a6d-fd74-d7a7d39b88d9"
      },
      "source": [
        "list_vocab = build_vocab(X_train);\n",
        "# Select randomly 20 vocab in X_train and the number of occurrences of it in X_train\n",
        "for i, val in enumerate(random.sample(list(list_vocab), 20)):\n",
        "    print(val + \": \" + str(list_vocab[val]))\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selfinterference: 8311\n",
            "hath: 35842\n",
            "degeneracybreaking: 26555\n",
            "singleplanet: 22596\n",
            "koreas: 39931\n",
            "pieta: 17447\n",
            "tiled: 35930\n",
            "shadowing: 17254\n",
            "flexural: 13652\n",
            "brbrightarrow: 39751\n",
            "beampolarization: 11121\n",
            "grouped: 7233\n",
            "pathogen: 11136\n",
            "tile: 8478\n",
            "radmcd: 28402\n",
            "ffcnn: 25442\n",
            "sfss: 25043\n",
            "myerson: 18344\n",
            "metaparticle: 36729\n",
            "energy: 758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dpXtjguinE3",
        "colab_type": "text"
      },
      "source": [
        "## 2. Building the Multinomial Naive Bayes model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_T0Jqiv1hjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_naive_bayes(texts, labels, target_classes, alpha=1):\n",
        "    \"\"\"\n",
        "    Train a multinomial Naive Bayes model\n",
        "    \"\"\"\n",
        "    ndoc = 0\n",
        "    nc = defaultdict(int)   # map from a class label to number of documents in the class\n",
        "    logprior = dict()\n",
        "    loglikelihood = dict()\n",
        "    count = defaultdict(int)  # count the occurrences of w in documents of class c\n",
        "\n",
        "    vocab = build_vocab(texts)\n",
        "    # Training\n",
        "    for s, c in zip(texts, labels):\n",
        "        ndoc += 1\n",
        "        nc[c] += 1\n",
        "        for w in s.split():\n",
        "            if w in vocab:\n",
        "                count[(w,c)] += 1\n",
        "    \n",
        "    vocab_size = len(vocab)\n",
        "    for c in target_classes:\n",
        "        logprior[c] = math.log(nc[c]/ndoc)\n",
        "        sum_ = 0\n",
        "        for w in vocab.keys():\n",
        "            if (w,c) not in count: count[(w,c)] = 0\n",
        "            sum_ += count[(w,c)]\n",
        "        \n",
        "        for w in vocab.keys():\n",
        "            loglikelihood[(w,c)] = math.log( (count[(w,c)] + alpha) / (sum_ + alpha * vocab_size) )\n",
        "    \n",
        "    return logprior, loglikelihood, vocab"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv2OuQl_i2G5",
        "colab_type": "text"
      },
      "source": [
        "Set the `target_classes` by adding ten unique categories of dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFyK5-2T-fsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba170d20-9ab0-49e6-dc34-acc4ac537791"
      },
      "source": [
        "target_classes = set()\n",
        "for index in Y_train:\n",
        "    target_classes.add(index)\n",
        "\n",
        "print(target_classes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'eess', 'hep-th', 'q-bio', 'cs', 'math', 'stat', 'cond-mat', 'hep-ph', 'quant-ph', 'astro-ph'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTdWuv7zjF6G",
        "colab_type": "text"
      },
      "source": [
        "### Time to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDcyA_D4rEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logprior, loglikelihood, vocab = train_naive_bayes(X_train, Y_train, target_classes)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ9Wv3GwjbVW",
        "colab_type": "text"
      },
      "source": [
        "## 3. Predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYhN_i1c48Am",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_naive_bayes(testdoc, logprior, loglikelihood, target_classes, vocab):\n",
        "    sum_ = {}\n",
        "    for c in  target_classes:\n",
        "        sum_[c] = logprior[c]\n",
        "        for w in testdoc.split():\n",
        "            if w in vocab:\n",
        "                sum_[c] += loglikelihood[(w,c)]\n",
        "    # sort keys in sum_ by value\n",
        "    sorted_keys = sorted(sum_.keys(), key=lambda x: sum_[x], reverse=True)\n",
        "    return sorted_keys[0]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MqOnfEMjkx-",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRY2wFI2lNX_",
        "colab_type": "text"
      },
      "source": [
        "## 1. Accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYAB9rVAU_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddb36c40-8161-4a20-eefc-a9de2b6ffd54"
      },
      "source": [
        "predicted_labels = [test_naive_bayes(s, logprior, loglikelihood, target_classes, vocab)\n",
        "                    for s in X_test]\n",
        "\n",
        "print('Accuracy score: %f' % metrics.accuracy_score(Y_test, predicted_labels))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.892000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshktihhlIj9",
        "colab_type": "text"
      },
      "source": [
        "## 2. Calculate precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXd6qr5NAqcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e09125b5-2a60-46ae-cdc8-d37259c63778"
      },
      "source": [
        "print('  Precision: %f' % metrics.precision_score(Y_test, predicted_labels, average='macro'))\n",
        "print('  Recall: %f' % metrics.recall_score(Y_test, predicted_labels, average='macro'))\n",
        "print('  F1: %f' % metrics.f1_score(Y_test, predicted_labels, average='macro'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Precision: 0.893277\n",
            "  Recall: 0.892000\n",
            "  F1: 0.892093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Srv9VjylTDn",
        "colab_type": "text"
      },
      "source": [
        "## 3. Compute macro-averaged and micro-averaged f1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ltye5v6CTIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7fef5dc-c9d7-4c9b-c15d-da404fc6d111"
      },
      "source": [
        "print('Macro-averaged f1: %f' % metrics.f1_score(Y_test, predicted_labels, average='macro'))\n",
        "print('Micro-averaged f1: %f' % metrics.f1_score(Y_test, predicted_labels, average='micro'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro-averaged f1: 0.892093\n",
            "Micro-averaged f1: 0.892000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6-qJfUUlXYH",
        "colab_type": "text"
      },
      "source": [
        "## 4. Report classification results all by once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09BdwPS9CQuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8a6628f3-a7e9-467b-a6cf-2f4a01efd6d5"
      },
      "source": [
        "print(metrics.classification_report(Y_test, predicted_labels, digits=3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    astro-ph      0.990     0.970     0.980       100\n",
            "    cond-mat      0.902     0.920     0.911       100\n",
            "          cs      0.738     0.790     0.763       100\n",
            "        eess      0.839     0.780     0.808       100\n",
            "      hep-ph      0.958     0.920     0.939       100\n",
            "      hep-th      0.892     0.910     0.901       100\n",
            "        math      0.902     0.830     0.865       100\n",
            "       q-bio      0.895     0.940     0.917       100\n",
            "    quant-ph      0.929     0.920     0.925       100\n",
            "        stat      0.887     0.940     0.913       100\n",
            "\n",
            "    accuracy                          0.892      1000\n",
            "   macro avg      0.893     0.892     0.892      1000\n",
            "weighted avg      0.893     0.892     0.892      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}